{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassificationOnText_EmotionDetection",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqF2ySHyUvWJ",
        "colab_type": "text"
      },
      "source": [
        "***Powered by:***\n",
        "\n",
        "![uniXerr logo](https://drive.google.com/uc?id=1TXJwfJsTJzU2M7LrIQgx2Tx4cfUzcQuX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXYXV4DFV-xQ",
        "colab_type": "text"
      },
      "source": [
        "# **MLP STRUCTURE**\n",
        "\n",
        "![mlp](https://drive.google.com/uc?id=1jeIYTAkZNMB31a5NalFhNSKMmx8-kBe-)\n",
        "\n",
        "# **FEED FORWARD**\n",
        "As we already know that in the feed froward we compute `A` and `dZ` such as\n",
        "`Z = X.W`\n",
        "`A = activation(Z) and dZ = dA = activationâ€™(Z)`.\n",
        "So to compute the dimensions of `A` and `dZ`, it suffices to look at the dimensions of `X` and `W`, which are `(m, 2)` and `(2, 4)` respectively.\n",
        "The result is `(m, 2) x (2, 4) = (m, 4)`.\n",
        "The following pictures depicts how the `A` and dZ are filled to form matrices of dimension `(m, 4)`.\n",
        "\n",
        "![feed forward](https://drive.google.com/uc?id=1mMKFqLtgNjoR-g8MZoGhDBIUTNwS8jjM)\n",
        "\n",
        "# **GRADIENT DESCENT ALGORITHM**\n",
        "\n",
        "![gda](https://drive.google.com/uc?id=17qkqt03Iv5gz1iWLHdNA4TuF8dGG7pFt)\n",
        "\n",
        "# **BACK PROPAGATION**\n",
        "As a reminder, the formulas in the Back Propagation are:\n",
        "\n",
        "`ðš«á´¸ = (Aá´¸ - Y) * dZá´¸`\n",
        "\n",
        "`ðš«â± = (ðš«â±âºÂ¹ . W áµ€)* dZâ±`\n",
        "\n",
        "(where * is a member wise multiplication)\n",
        "We know by now that both `A` and `dZ` have the dimensions `(m, n)` where m is the number of samples and n is the number of nodes in the layer. So the `ðš«á´¸` will have the same dimension `(m, n)`\n",
        "In our example the last layer (L) has only one node, so `A` and `dZ` at layer L have the dimension `(m, 1)`. It follows that `ðš«á´¸` has also the dimension `(m, 1)`.\n",
        "In order to compute the deltas of the inner layers, letâ€™s take layers 2 and 3 of the example.\n",
        "\n",
        "![bp](https://drive.google.com/uc?id=1t6AP_RnQHVab8H_iRh1PKlfYTw4P8kiP)\n",
        "\n",
        "The weight `ðž±` between layer 2 and layer 3 has the dimension `(4, 3)`, since the 4 nodes of layer 2 are connected to 3 nodes of layers 3.\n",
        "\n",
        "![theta](https://drive.google.com/uc?id=1jxXV5sWLH_-uKMiAS4DVBlhKt_XC3-kk)\n",
        "\n",
        "However since the delta at layer 2 is the dot product of the delta coming from layer 3 and the weight `ðž±`, we end up with the following configuration.\n",
        "\n",
        "![bp mat](https://drive.google.com/uc?id=1hK3tOL3VT5M32RSkHb4Il1LJHirXFgFj)\n",
        "\n",
        "It not possible to do the dot product and obtain delta at layer 2 with dimension `(m, 4)`!\n",
        "To solve this, we use `ðž±áµ€` the transpose of `ðž±`.\n",
        "The computation will be possible as shown by the images below\n",
        "\n",
        "![bp matt](https://drive.google.com/uc?id=1aINCIzkYNtorAngdp4epCxT2vIJu3U0l)\n",
        "\n",
        "Updating the weights at layer i, involves getting the input at the layer, perform the dot product with the delta of the layer then use the result to update the weights.\n",
        "If we take layers 1 and 2 of our example, we have an input, at layer 1, with dimension `(m, 2)` and the dimension of delta at layer 2 `(m, 4)`.\n",
        "However the weights matrix connecting layer 1 and 2 has the dimension\n",
        "`(2, 4)`.\n",
        "\n",
        "![bp mat](https://drive.google.com/uc?id=1DPImpBwwKEhZAOLkHrImu60vZygbj8xw)\n",
        "\n",
        "To be able to update the weights matrix, the dot product between `X` and `ðš«` should result in a `(2, 3)` matrix.\n",
        "This is done by getting the transpose of `X` that is `Xáµ€`, then perform the dot product `Xáµ€` . `ðš«` that we call `ðž­`.\n",
        "\n",
        "![bp full mat comp](https://drive.google.com/uc?id=1ILL_wpGOHo-NKFCD7bsUEWJ3_YYKFbDH)\n",
        "\n",
        "Now we can update the weight matrix using the gradient descent formula\n",
        "`Wâ¿âºÂ¹ = Wâ¿ - ð° * ðž­` (in here n is the version of W and not the layer index).\n",
        "\n",
        "*Credit:* [Ziad SALLOUM](https://towardsdatascience.com/@zsalloum?source=post_page-----cc1de33e8397----------------------)\n",
        "\n",
        "# **CONFUSION MATRIX**\n",
        "\n",
        "In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabelling one as another).\n",
        "\n",
        "![confusion visualization](https://drive.google.com/uc?id=1eVNJniz_QXc3MsPerNlz0XZEgK9DnE8C)\n",
        "\n",
        "**Accuracy**â€Šâ€”â€ŠAccuracy refers to the closeness of a measured value to a standard or known value.\n",
        "\n",
        "**Precision**â€Šâ€”â€ŠPrecision refers to the closeness of two or more measurements to each other. It is the repeatability or reproducibility of the measurement.\n",
        "\n",
        "**Recall(Sensitivity)**â€Šâ€”â€ŠRecall refers to the fraction of relevant instances that have been retrieved over the total amount of relevant instances.\n",
        "\n",
        "# **BATCHES , EPOCH AND ITERATION**\n",
        "\n",
        "In Deep Learning, an epoch is a hyperparameter which is defined before training a model. One epoch is when an entire dataset is passed both forward and backward through the neural network only once.\n",
        "\n",
        "One epoch is too big to feed to the computer at once. So, we divide it in several smaller batches. We use more than one epoch because passing the entire dataset through a neural network is not enough and we need to pass the full dataset multiple times to the same neural network. But since we are using a limited dataset and to optimise the learning and the graph we are using Gradient Descent which is an iterative process. So, updating the weights with single pass or one epoch is not enough.\n",
        "\n",
        "A batch is the total number of training examples present in a single batch and an iteration is the number of batches needed to complete one epoch.\n",
        "\n",
        "For example: If we divide a dataset of 2000 training examples into 500 batches, then 4 iterations will complete 1 epoch.\n",
        "The weights will be updated at the end of each training epoch.\n",
        "\n",
        "![epoch_batches_training](https://qph.fs.quoracdn.net/main-qimg-5e4fa1bbf4c0138642d0d8bd298ef09f)\n",
        "\n",
        "\n",
        "\n",
        "# **LSTM INPUT SHAPE**\n",
        "[LSTM explained and its diagrams](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)\n",
        "\n",
        "![lsmt_d1](https://miro.medium.com/max/700/1*laH0_xXEkFE0lKJu54gkFQ.png)\n",
        "\n",
        "*X0 , X1 , .... , X30 is timesteps which in our case is 20 inputs (word) which every Xi has a length of 50 and our lstm has 100 layers with none batches (cause we don't have rows in our data and the data is just a vector with size 50 for each input (word) ).\n",
        "20 is the time steps per sample which means every sample has 20 rows ; every text is padded by 20 (20 words maximum) and is the length of each sequence(sentence) in each sample ; so for every sample we have a (20 * 50) matrix full of a vector(features) with size 50 for each word. in total we have a tensor with size (30000*20*50). \n",
        "*\n",
        "\n",
        "```\n",
        "N=1\n",
        "model = Sequential()\n",
        "model.add(LSTM(N))\n",
        "```\n",
        "\n",
        "![types of lstm](https://i.stack.imgur.com/xLZCK.png)\n",
        "*For the other models you would need N>1*\n",
        "![types of lstm](https://i.stack.imgur.com/PQs02.png)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "<--- Many To One With Multiple Features [input_shape = (20, 50)] --->\n",
        "\n",
        "[ -> size = 30000 = 30000 samples\n",
        "    [ -> size = 20 = 20 rows = time_steps\n",
        "        [........] -> size = 50 = 50 features\n",
        "        [........]\n",
        "            .\n",
        "            .\n",
        "            .\n",
        "        [........]\n",
        "        [........]\n",
        "    ]\n",
        "\n",
        "    [ -> size = 20 = 20 rows = time_steps\n",
        "        [........] -> size = 50 = 50 features\n",
        "        [........]\n",
        "            .\n",
        "            .\n",
        "            .\n",
        "        [........]\n",
        "        [........]\n",
        "    ]\n",
        "\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "\n",
        "    [ -> size = 20 = 20 rows = time_steps\n",
        "        [........] -> size = 50 = 50 features\n",
        "        [........]\n",
        "            .\n",
        "            .\n",
        "            .\n",
        "        [........]\n",
        "        [........]\n",
        "    ]\n",
        "]\n",
        "```\n",
        "\n",
        "[LSTM diagrams and its text classification example](https://adventuresinmachinelearning.com/keras-lstm-tutorial/)\n",
        "\n",
        "![lstm_d2](https://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-architecture.png)\n",
        "\n",
        "\n",
        "*The input of the LSTM is always is a 3D array. (batch_size, time_steps, seq_len)\n",
        "The output of the LSTM could be a 2D array or 3D array depending upon the return_sequences argument.\n",
        "If return_sequence is False, the output is a 2D array. (batch_size, units)\n",
        "If return_sequence is True, the output is a 3D array. (batch_size, time_steps, units)\n",
        "You always have to give a three-dimensional array as an input to your LSTM network (refer to the above image). Where the first dimension represents the batch size, the second dimension represents the number of time-steps you are feeding a sequence. And the third dimension represents the number of units in one input sequence. For example, input shape looks like (batch_size, time_steps, seq_len).*\n",
        "\n",
        "[LSTM input & output shape](https://medium.com/@shivajbd/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e)\n",
        "\n",
        "![lstm_d3](https://miro.medium.com/max/532/1*v5_QpzkQfufVogeCY9eaOw.png)\n",
        "\n",
        "\n",
        "# **What is the Difference Between a 1D CNN and a 2D CNN?**\n",
        "CNNs work the same way whether they have 1, 2, or 3 dimensions. The difference is the structure of the input data and how the filter, also known as a convolution kernel or feature detector, moves across the data.\n",
        "![conv1D](https://missinglink.ai/wp-content/uploads/2019/03/1D-convolutional-example_2x.png)\n",
        "\n",
        "*In this natural language processing (NLP) example, a sentence is made up of 9 words. Each word is a vector that represents a word. The filter covers at least one word; a height parameter specifies how many words the filter should consider at once. In this example the height is 2, meaning the filter moves 8 times to fully scan the data.*\n",
        "\n",
        "![conv2D](https://missinglink.ai/wp-content/uploads/2019/03/2D-convolutional-example_2x-700x584.png)\n",
        "\n",
        "*In a 2D convolutional network, each pixel within the image is represented by its x and y position as well as the depth, representing image channels (red, green, and blue). The filter in this example is 2Ã—2 pixels. It moves over the images both horizontally and vertically.\n",
        "Another difference between 1D and 2D networks is that 1D networks allow you to use larger filter sizes. In a 1D network, a filter of size 7 or 9 contains only 7 or 9 feature vectors. Whereas in a 2D CNN, a filter of size 7 will contain 49 feature vectors, making it a very broad selection.\n",
        "Another difference, though, is the fact that you can afford to use larger convolution windows with 1D CNNs. With a 2D convolution layer, a 3 Ã— 3 convolution window contains 3 Ã— 3 = 9 feature vectors. With 1D convolution layer, a window of size 3 contains only 3 feature vectors. You can thus easily afford 1D convolution windows of size 7 or 9.*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **BUILDING MODEL**\n",
        "![embedding_matrix](https://miro.medium.com/max/700/1*zaXqnINwmKfac2lrNqjTVw.png)\n",
        "\n",
        "\n",
        "![model](http://www.upsara.com/images/z50950_ted_NN_model.png)\n",
        "\n",
        "---\n",
        "\n",
        "# **TRAINING , VALID AND TESTING DATA**\n",
        "If you want to build a solid model you have to follow that specific protocol of \n",
        "splitting your data into three sets: One for training, \n",
        "one for validation and one for final evalution, which is the test set.\n",
        "The idea is that you train on your training data and tune your \n",
        "model with the results of metrics (accuracy, loss etc) that you get from your validation set.\n",
        "Your model doesn't \"see\" your validation set and isnÂ´t in any way trained on it, \n",
        "but you as the architect and master of the hyperparameters tune the model according to this data. \n",
        "Therefore it indirectly influences your model because it directly influences \n",
        "your design decisions. You nudge your model to work well with the validation \n",
        "data and that can possibly bring in a tilt.\n",
        "Exactly that is the reason you only evaluate your models final score on data \n",
        "that neither your model nor you yourself has used â€“ and that is the third chunk of data, your test set.\n",
        "Only this procedure makes sure you get an unaffected view of your models quality\n",
        "and ability to generalize what is has learned on totally unseen data.\n",
        "\n",
        "\n",
        "# **Overfitting**\n",
        "Overfitting occurs when a statistical model or machine learning algorithm captures the noise of the data.  \n",
        "Intuitively, overfitting occurs when the model or the algorithm fits the data too well.  \n",
        "Specifically, overfitting occurs if the model or algorithm shows low bias but high variance.  \n",
        "Overfitting is often a result of an excessively complicated model, and it can be prevented by \n",
        "fitting multiple models and using validation or cross-validation to compare their predictive accuracies on test data.\n",
        "When training a machine learning model, one of the main things that \n",
        "you want to avoid would be overfitting. This is when your model fits \n",
        "the training data well, but it isn't able to generalize and make \n",
        "accurate predictions for data it hasn't seen before.\n",
        "To find out if their model is overfitting, data scientists \n",
        "use a technique called cross-validation, where they split their \n",
        "data into two parts - the training set, and the validation set. \n",
        "The training set is used to train the model, while the validation set \n",
        "is only used to evaluate the model's performance.\n",
        "Metrics on the training set let you see how your model is progressing in \n",
        "terms of it's training, but it's metrics on the validation set that let \n",
        "you get a measure of the quality of your model - how well it's able to \n",
        "make new predictions based on data it hasn't seen before.\n",
        "With this in mind, loss and acc are measures of loss and accuracy on \n",
        "the training set, while val_loss and val_acc are measures of loss and accuracy \n",
        "on the validation set. At the moment the model has an accuracy of ~38% on the \n",
        "training set and ~40% on the validation set. This means that you can expect your \n",
        "model to perform with ~40% accuracy on new data.\n",
        "as the epochs goes from 10 to 100, your acc metric increases, \n",
        "while your val_acc metric decreases. This means that your model is fitting the \n",
        "training set better, but is losing it's ability to predict on new data, \n",
        "indicating that your model is starting to fit on noise and is beginning to overfit.\n",
        "\n",
        "\n",
        "# **Underfitting**\n",
        "Underfitting occurs when a statistical model or machine learning algorithm cannot \n",
        "capture the underlying trend of the data.  Intuitively, underfitting occurs when \n",
        "the model or the algorithm does not fit the data well enough. Specifically, \n",
        "underfitting occurs if the model or algorithm shows low variance but high bias.  \n",
        "Underfitting is often a result of an excessively simple model.\n",
        "\n",
        "\n",
        "# **Tricks**\n",
        "If your training loss is much lower than validation loss then \n",
        "this means the network might be overfitting. Solutions to this \n",
        "are to decrease your network size, or to increase dropout. \n",
        "For example you could try dropout of 0.5 and so on.\n",
        "If your training/validation loss are about equal then your model \n",
        "is underfitting. Increase the size of your model \n",
        "(either number of layers or the raw number of neurons per layer)\n",
        "a typical reason for validation accuracy being lower than \n",
        "training accuracy was overfitting. when the opposite is true \n",
        "itâ€™s because the model is underfitting the data.\n",
        "\n",
        "\n",
        " \n",
        "```\n",
        ".\n",
        ".\n",
        ".\n",
        ".\n",
        "\n",
        "# Epoch 100/100\n",
        "# 29936/29936 [==============================] - 10s 326us/step - loss: 1.8454 - acc: 0.3736 - val_loss: 1.7907 - val_acc: 0.3906\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**in all epochs we have underfitting , due to this problem our  model comes into underfitting area , cause we don't have enough dataset!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiUWuOVT9tTx",
        "colab_type": "text"
      },
      "source": [
        "# **TEXT CLASSIFICATION - EMOTION DETECTION KERAS CODE**\n",
        "*`getting our hands on`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lyBdapZO9xrm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d13e0592-9060-428e-e44b-f9d9fb549f6d"
      },
      "source": [
        "# -*- encoding: utf-8 -*-\n",
        "\n",
        "\n",
        "import numpy as np, pandas as pd, os, sys, time, datetime, nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential, load_model, save_model, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten, LSTM, Bidirectional, Embedding, Flatten, Conv1D, MaxPooling1D\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "drive.mount('/gdrive')\n",
        "nltk.download('stopwords')\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "\n",
        "# ===================================\n",
        "# Preprocessing and formating\n",
        "# ===================================\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 20\n",
        "EMBEDDING_DIM = 50\n",
        "dataFrame = pd.read_csv('/gdrive/My Drive/text_emotion_twitter.csv', encoding='utf-8')\n",
        "# dataFrame is a matrix of 40000 rows X 4 cols\n",
        "# dataFrame.values[[rows], [cols]]\n",
        "x = dataFrame.values[:,3] # accessing third elements of all rows which is content/text => 40000 texts/contents\n",
        "y = dataFrame.values[:,1] # accessing first elements of all rows which is label/emotions => 40000 labels/emotions\n",
        "print(\"[+] data size from csv >>> \", x.shape)\n",
        "print(\"[+] label size from csv >>> \", y.shape)\n",
        "\n",
        "\n",
        "def uprint(*objects, sep=' ', end='\\n', file=sys.stdout):\n",
        "    enc = file.encoding\n",
        "    if enc == 'UTF-8':\n",
        "        print(*objects, sep=sep, end=end, file=file)\n",
        "    else:\n",
        "        f = lambda obj: str(obj).encode(enc, errors='backslashreplace').decode(enc)\n",
        "        print(*map(f, objects), sep=sep, end=end, file=file)\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize = (12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label = 'Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label = 'Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label = 'Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label = 'Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "new_stop_words = set(stop_words)\n",
        "# adding woudlnt type of words into stopwords list\n",
        "for s in stop_words:\n",
        "    new_stop_words.add(s.replace('\\'',''))\n",
        "    pass\n",
        "stop_words = new_stop_words\n",
        "print(\"\\n[+] Excluding stopwords ...\")\n",
        "\n",
        "\n",
        "# removing @ from default base filter, to remove that whole word, which might be considered as user or page name\n",
        "base_filters = '\\n\\t!\"#$%&()*+,-./:;<=>?[\\]^_`{|}~ '\n",
        "word_sequences = []\n",
        "for i in x:\n",
        "    i = str(i)\n",
        "#     uprint(i)\n",
        "    i = i.replace('\\'', '')\n",
        "    newlist = [x for x in text_to_word_sequence(i,filters = base_filters, lower = True) if not x.startswith(\"@\")]\n",
        "    filtered_sentence = [w for w in newlist if not w in stop_words] \n",
        "    word_sequences.append(filtered_sentence)\n",
        "    pass\n",
        "# print(\"\\n[+] word sequences >>> \", word_sequences) # len is : 40000\n",
        "# [ ['know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part'], ....... , ['layin', 'n', 'bed', 'headache', 'ughhhh', 'waitin', 'call'] ]\n",
        "\n",
        "\n",
        "\n",
        "# Tokenizing words to word indices\n",
        "tokenizer = Tokenizer()\n",
        "# fitting texts to tokenizer object\n",
        "tokenizer.fit_on_texts(word_sequences)\n",
        "word_indices = tokenizer.texts_to_sequences(word_sequences)\n",
        "word_index = tokenizer.word_index # size is 32855 cause its a dict with key as word and index as value\n",
        "print(\"\\n[+] Saving word_index into picke.... \")\n",
        "with open(\"/gdrive/My Drive/ted/word_index.pkl\",\"wb\") as f:\n",
        "    pickle.dump(word_index, f)\n",
        "# print(\"\\n[+] word index >>> \", word_index) \n",
        "# word_index : { 'im': 1, 'day': 2, 'good': 3, 'get': 4, 'like': 5, ......  , 'quot': 6, 'http': 7, 'go': 8, 'today': 9 }\n",
        "print(\"\\n[+] Tokenized to Word indices as \")\n",
        "print(np.array(word_indices).shape) # size is 40000\n",
        "# print(\"\\n[+] word indices >>> \", word_indices)\n",
        "# word_indices : [ [20, 3077, 57, 4396, 714, 489, 1014, 433], ...... , [8964, 177, 74, 319, 3327, 2077, 192], [2555, 3963, 1598, 143] ]\n",
        "\n",
        "\n",
        "\n",
        "# padding word_indices\n",
        "# >>> pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]])\n",
        "# array([[0, 1, 2, 3],\n",
        "#        [3, 4, 5, 6],\n",
        "#        [0, 0, 7, 8]], dtype=int32)\n",
        "\n",
        "x_data = pad_sequences(word_indices, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print(\"\\n[+] After padding data size is\")\n",
        "print(x_data.shape) # (40000, 20)\n",
        "print(\"\\n[+] padded x_data >>> \")\n",
        "print(x_data)\n",
        "# [[    0     0     0 ...   489  1014   433]\n",
        "#  [    0     0     0 ...  3327  2077   192]\n",
        "#  [    0     0     0 ...  3963  1598   143]\n",
        "#  ...\n",
        "#  [    0     0     0 ...  1390   165     2]\n",
        "#  [    0     0     0 ...  1135    19   336]\n",
        "#  [    0     0     0 ... 32852 32853 32854]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===================================\n",
        "# Building Embedding Layer\n",
        "# ===================================\n",
        "\n",
        "\n",
        "# using pretrained glove vector\n",
        "print(\"\\n[+] Loading Glove Vectors ...\")\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('', '/gdrive/My Drive/glove.6B.50d.txt'),'r',encoding=\"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('\\n[+] Loaded GloVe Vectors Successfully!')\n",
        "\n",
        "\n",
        "# embedding_matrix size is : vocab + 1 X 50\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(\"\\n[+] Embedding Matrix Generated with size >>>> \", embedding_matrix.shape)\n",
        "print(\"\\n[+] embedding matrix is \")\n",
        "print(embedding_matrix)\n",
        "# it has len(word_index) + 1 rows and 50 cols which is (32855, 50)\n",
        "# [[ 0.          0.          0.         ...  0.          0.\n",
        "#    0.        ]\n",
        "#  [-0.067678    0.51832002  1.32599998 ... -0.65103     0.12924001\n",
        "#    0.48723999]\n",
        "#  [ 0.11626     0.53896999 -0.39513999 ... -0.39061999 -0.10885\n",
        "#    0.084513  ]\n",
        "#  ...\n",
        "#  [ 1.18879998  1.46720004 -0.99624002 ...  0.48704001  0.77978998\n",
        "#    0.38242999]\n",
        "#  [ 0.013849   -0.54549998 -0.077683   ...  0.67878002  0.46202999\n",
        "#    0.72376001]\n",
        "#  [ 0.023778   -0.80124003  0.1193     ... -1.06830001 -0.35703\n",
        "#    0.21013001]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This layer acts as lookup table for vectors, given word index. \n",
        "It will return embedded word vector.\n",
        "Embedding layer can only be used as first layer in Keras.\n",
        "Our input layer will of be size : (None,20) ; None means variable number.\n",
        "As we have padded 20 words for each input, in data preparation stage. \n",
        "we have 20 word indices in each row.\n",
        "Output of Embedding layer will be fed to this LSTM layer.\n",
        "\n",
        "WATNING : we need very large dataset to get a vector for our word , because of embedding layer manner and tokenizer algorithm\n",
        "          we can only get the vector of those words which exists in our dataset and anything else will fall into fail\n",
        "          cause our new word doesn't exit in our word_index(vocab) so it won't give us its index and all we get is 0 vector or none!\n",
        "\"\"\"\n",
        "embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
        "\n",
        "\n",
        "# ===================================\n",
        "# One Hot encoding Labels\n",
        "# ===================================\n",
        "\n",
        "\n",
        "# Example from sklearn API :\n",
        "\n",
        "# >>> le = preprocessing.LabelEncoder()\n",
        "# >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
        "# LabelEncoder()\n",
        "# >>> list(le.classes_)\n",
        "# ['amsterdam', 'paris', 'tokyo']\n",
        "# >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) \n",
        "# array([2, 2, 1]...)\n",
        "# >>> list(le.inverse_transform([2, 2, 1]))\n",
        "# ['tokyo', 'tokyo', 'paris']\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(y) # Fit label encoder and return encoded labels\n",
        "# print(label_encoder.classes_)\n",
        "# print(label_encoder.transform(label_encoder.classes_)) \n",
        "\n",
        "le_name_mapping = dict(zip(label_encoder.transform(label_encoder.classes_),label_encoder.classes_))\n",
        "print(\"\\n[+] Label Encoding Classes as \")\n",
        "print(le_name_mapping)\n",
        "# {0: 'anger', 1: 'boredom', 2: 'empty', 3: 'enthusiasm', 4: 'fun', 5: 'happiness', 6: 'hate', 7: 'love', 8: 'neutral', 9: 'relief', 10: 'sadness', 11: 'surprise', 12: 'worry'}\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "to_categorical\n",
        "\n",
        "Converts a class vector (integers) to binary class matrix.\n",
        "\n",
        "Arguments\n",
        "\n",
        "y: class vector to be converted into a matrix (integers from 0 to num_classes).\n",
        "num_classes: total number of classes.\n",
        "dtype: The data type expected by the input, as a string (float32, float64, int32...)\n",
        "\n",
        "Return\n",
        "\n",
        "A binary matrix representation of the input. The classes axis is placed last.\n",
        "\n",
        "Example\n",
        "\n",
        "# Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:\n",
        "> labels\n",
        "array([0, 2, 1, 2, 0])\n",
        "# `to_categorical` converts this into a matrix with as many\n",
        "# columns as there are classes. The number of rows\n",
        "# stays the same.\n",
        "> to_categorical(labels)\n",
        "array([[ 1.,  0.,  0.],\n",
        "       [ 0.,  0.,  1.],\n",
        "       [ 0.,  1.,  0.],\n",
        "       [ 0.,  0.,  1.],\n",
        "       [ 1.,  0.,  0.]], dtype=float32)\n",
        "       \n",
        "\"\"\"\n",
        "\n",
        "y_data = np_utils.to_categorical(integer_encoded)\n",
        "print(\"\\n[+] One Hot Encoded class shape with size \")\n",
        "print(y_data.shape) # (40000 , 13)\n",
        "print(\"\\n[+] One hot encoded labels matrix\")\n",
        "print(y_data)\n",
        "# [[0. 0. 1. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 1. 0. 0.]\n",
        "#  [0. 0. 0. ... 1. 0. 0.]\n",
        "#  ...\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===================================\n",
        "# Building Model\n",
        "# ===================================\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Weâ€™ll use LSTM layer with 100 units. This layer has 100 RNN Cells, \n",
        "this number is variable and can be adjusted according to our need and complexity of our data.\n",
        "Input given to LSTM will be considered as (batch_size, timesteps, features).\n",
        "\n",
        "samples/batch_size. These are the rows in your data : is None , cause our data is a vector with size 50 in each input(timestep)\n",
        "timesteps. These are the past observations for a feature, such as lag variables. : 20 is the number of input cause we padded each word with 20 length , \n",
        "                                                                                   20 words(timesteps) 50D(feature) vector with none rows(batch_size) for each text\n",
        "features. These are columns in your data : 50 , as the dim of each word in each input\n",
        "\n",
        "-----------------------\n",
        "return_sequences=True :\n",
        "-----------------------\n",
        "Output of RNN layer will include all the outputs from all the units/cells in that layer.\n",
        "(None, 20,50) => LSTM(100, return_sequences=True) => (None, 20, 100)\n",
        "\n",
        "[NOTE] : batch_size is None cause we don't have rows in our data (our data is a vector with size 50 for each input(timestep))\n",
        "\n",
        "In the next step, weâ€™ll flatten.\n",
        "(None, 20,100) => Flatten => (None,2000)\n",
        "If you wish to connect a Dense layer directly to an Embedding layer, \n",
        "you must first flatten the 2D output matrix to a 1D vector using the Flatten layer.\n",
        "\n",
        "-----------------------------------------\n",
        "Understanding Dense Layer (Last Layers) :\n",
        "-----------------------------------------\n",
        "We connect all the data that we get from previous levels using Dense Layers.. \n",
        "We keep reducing output units to (None, labels_count) by adding multiple Dense Layers.\n",
        "(None, 2000) => Dense(300) => (None,300)\n",
        "Adding another dense layer\n",
        "(None,300) => Dense(labels_count) => (None,13)\n",
        "13 is labelsâ€™ count in our problem. i.e Total number of emotions count.\n",
        "Softmax is probability distribution activation function and helps \n",
        "in achieving better results by distributing probability among labels for a given input.\n",
        "After adding this, we get 13 Outputs each lying between 0 and 1 for each input. \n",
        "Each output represents probability of that emotion for given input. One with highest value can be considered as our prediction.\n",
        "\n",
        "--------\n",
        "Conv1D :\n",
        "--------\n",
        "CNNs work the same way whether they have 1, 2, or 3 dimensions. \n",
        "The difference is the structure of the input data and how the filter, \n",
        "also known as a convolution kernel or feature detector, moves across the data.\n",
        "A 1D CNN is very effective for deriving features from a fixed-length segment \n",
        "of the overall dataset, where it is not so important where the feature is located in the segment.\n",
        "\"\"\"\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Conv1D(30, 1, activation=\"relu\")) # output size : (None, 20, 30)\n",
        "model.add(MaxPooling1D(4)) # output size : (None, 5, 50) - 20 / MaxPoolling1D size = 5\n",
        "model.add(LSTM(100, return_sequences=True)) # the output is the output of the last cell\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(300, activation='relu'))\n",
        "model.add(Dense(y_data.shape[1], activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "print(\"Finished Preprocessing data ...\")\n",
        "print(\"[+] x_data shape >>> \", x_data.shape)\n",
        "print(\"[+] y_data shape >>> \", y_data.shape)\n",
        "\n",
        "\n",
        "\n",
        "# We gotta split our data into two parts, Training Data, Testing Data\n",
        "# We use Training dataset to train our neural network. \n",
        "# Test dataset to provide an unbiased evaluation of a final model fit on the training dataset.\n",
        "# Split arrays or matrices into random train and test subsets\n",
        "print(\"[+] spliting data into training, testing set ...\")\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data)\n",
        "\n",
        "\n",
        "# # x_train\n",
        "# [[    0     0     0 ...     0    93   866]\n",
        "#  [    0     0     0 ...  1498  5908  7727]\n",
        "#  [    0     0     0 ...   565  2943   162]\n",
        "#  ...\n",
        "#  [    0     0     0 ...  1056 10331 24539]\n",
        "#  [    0     0     0 ...   520   995    71]\n",
        "#  [    0     0     0 ...   565   789   129]] \n",
        "# # x_test\n",
        "# [[    0     0     0 ...    49   595  2060]\n",
        "#  [    0     0     0 ...  2583  3626  2401]\n",
        "#  [    0     0     0 ...  3423  2463   398]\n",
        "#  ...\n",
        "#  [    0     0     0 ...   124    11 18737]\n",
        "#  [    0     0     0 ...    69  8389  8389]\n",
        "#  [    0     0     0 ...    15    27  5723]] \n",
        "# # y_train\n",
        "# [[0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  ...\n",
        "#  [0. 0. 0. ... 0. 0. 1.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]] \n",
        "# # y_test\n",
        "# [[0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 1. 0. 0.]\n",
        "#  ...\n",
        "#  [0. 0. 0. ... 0. 0. 1.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "num_epochs = 100\n",
        "\n",
        "\n",
        "x_valid, y_valid = x_train[:batch_size], y_train[:batch_size]\n",
        "x_train2, y_train2 = x_train[batch_size:], y_train[batch_size:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Save the model after every epoch.\n",
        "filepath can contain named formatting options, which will be filled \n",
        "with the values of epoch and keys in logs (passed in on_epoch_end).\n",
        "For example: if filepath is weights.{epoch:02d}-{val_loss:.2f}.hdf5, \n",
        "then the model checkpoints will be saved with the epoch number and the validation loss in the filename.\n",
        "\n",
        "Arguments\n",
        "\n",
        "filepath: string, path to save the model file.\n",
        "monitor: quantity to monitor.\n",
        "verbose: verbosity mode, 0 or 1.\n",
        "save_best_only: if save_best_only=True, the latest best model according \n",
        "    to the quantity monitored will not be overwritten.\n",
        "    save_weights_only: if True, then only the model's weights will be saved \n",
        "    (model.save_weights(filepath)), else the full model is saved (model.save(filepath)).\n",
        "mode: one of {auto, min, max}. If save_best_only=True, the decision to overwrite \n",
        "    the current save file is made based on either the maximization or the minimization \n",
        "    of the monitored quantity. For val_acc, this should be max, for val_loss this \n",
        "    should be min, etc. In auto mode, the direction is automatically inferred \n",
        "    from the name of the monitored quantity.\n",
        "period: Interval (number of epochs) between checkpoints.\n",
        "\"\"\"\n",
        "\n",
        "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(f\"[+] start time to train >>> {st}\")\n",
        "\n",
        "# Now, we define check point conditions, \n",
        "# these checkpoints will save our model locally if thereâ€™s an improvement.\n",
        "filepath = \"/gdrive/My Drive/ted/ed-{epoch:02d}-{val_acc:.6f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'auto', save_weights_only = False)\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "# fit x_train and y_train data to our model to train on them based on x_valid and y_valid \n",
        "history = model.fit(x_train2, y_train2, validation_data = (x_valid, y_valid), batch_size = batch_size, epochs = num_epochs, callbacks = callbacks_list)\n",
        "plot_history(history)\n",
        "\n",
        "\n",
        "ft = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(f\"[+] stop time for training >>> {ft}\")\n",
        "\n",
        "# evaluate model on the test data \n",
        "scores = model.evaluate(x_test, y_test, verbose = 0)\n",
        "tr_scores = model.evaluate(x_train, y_train, verbose = False)\n",
        "print(\"[+] Training Accuracy : {:.6f}\".format(tr_scores[1]))\n",
        "print(\"[+] Training Loss     : {:.6f}\".format(tr_scores[0]))\n",
        "te_scores = model.evaluate(x_test, y_test, verbose = False)\n",
        "print(\"[+] Testing Accuracy  : {:.6f}\".format(te_scores[1]))\n",
        "print(\"[+] Testing Loss      : {:.6f}\".format(tr_scores[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[+] data size from csv >>>  (40000,)\n",
            "[+] label size from csv >>>  (40000,)\n",
            "\n",
            "[+] Excluding stopwords ...\n",
            "\n",
            "[+] Saving word_index into picke.... \n",
            "\n",
            "[+] Tokenized to Word indices as \n",
            "(40000,)\n",
            "\n",
            "[+] After padding data size is\n",
            "(40000, 20)\n",
            "\n",
            "[+] padded x_data >>> \n",
            "[[    0     0     0 ...   489  1014   433]\n",
            " [    0     0     0 ...  3327  2077   192]\n",
            " [    0     0     0 ...  3963  1598   143]\n",
            " ...\n",
            " [    0     0     0 ...  1390   165     2]\n",
            " [    0     0     0 ...  1135    19   336]\n",
            " [    0     0     0 ... 32852 32853 32854]]\n",
            "\n",
            "[+] Loading Glove Vectors ...\n",
            "\n",
            "[+] Loaded GloVe Vectors Successfully!\n",
            "\n",
            "[+] Embedding Matrix Generated with size >>>>  (32855, 50)\n",
            "\n",
            "[+] embedding matrix is \n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-0.067678    0.51832002  1.32599998 ... -0.65103     0.12924001\n",
            "   0.48723999]\n",
            " [ 0.11626     0.53896999 -0.39513999 ... -0.39061999 -0.10885\n",
            "   0.084513  ]\n",
            " ...\n",
            " [ 1.18879998  1.46720004 -0.99624002 ...  0.48704001  0.77978998\n",
            "   0.38242999]\n",
            " [ 0.013849   -0.54549998 -0.077683   ...  0.67878002  0.46202999\n",
            "   0.72376001]\n",
            " [ 0.023778   -0.80124003  0.1193     ... -1.06830001 -0.35703\n",
            "   0.21013001]]\n",
            "\n",
            "[+] Label Encoding Classes as \n",
            "{0: 'anger', 1: 'boredom', 2: 'empty', 3: 'enthusiasm', 4: 'fun', 5: 'happiness', 6: 'hate', 7: 'love', 8: 'neutral', 9: 'relief', 10: 'sadness', 11: 'surprise', 12: 'worry'}\n",
            "\n",
            "[+] One Hot Encoded class shape with size \n",
            "(40000, 13)\n",
            "\n",
            "[+] One hot encoded labels matrix\n",
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 20, 50)            1642750   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 20, 30)            1530      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 30)             0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 5, 100)            52400     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 300)               150300    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 13)                3913      \n",
            "=================================================================\n",
            "Total params: 2,101,393\n",
            "Trainable params: 458,643\n",
            "Non-trainable params: 1,642,750\n",
            "_________________________________________________________________\n",
            "None\n",
            "Finished Preprocessing data ...\n",
            "[+] x_data shape >>>  (40000, 20)\n",
            "[+] y_data shape >>>  (40000, 13)\n",
            "[+] spliting data into training, testing set ...\n",
            "[+] start time to train >>> 2019-07-17 20:06:52\n",
            "Train on 29936 samples, validate on 64 samples\n",
            "Epoch 1/100\n",
            "29936/29936 [==============================] - 40s 1ms/step - loss: 2.2623 - acc: 0.2257 - val_loss: 2.0778 - val_acc: 0.1875\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.18750, saving model to /gdrive/My Drive/ted/ed-01-0.187500.hdf5\n",
            "Epoch 2/100\n",
            "29936/29936 [==============================] - 36s 1ms/step - loss: 2.1503 - acc: 0.2495 - val_loss: 2.0512 - val_acc: 0.2031\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.18750 to 0.20312, saving model to /gdrive/My Drive/ted/ed-02-0.203125.hdf5\n",
            "Epoch 3/100\n",
            "29936/29936 [==============================] - 37s 1ms/step - loss: 2.1359 - acc: 0.2521 - val_loss: 2.0364 - val_acc: 0.2188\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.20312 to 0.21875, saving model to /gdrive/My Drive/ted/ed-03-0.218750.hdf5\n",
            "Epoch 4/100\n",
            "29936/29936 [==============================] - 36s 1ms/step - loss: 2.1270 - acc: 0.2530 - val_loss: 2.0260 - val_acc: 0.2188\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.21875\n",
            "Epoch 5/100\n",
            "29936/29936 [==============================] - 36s 1ms/step - loss: 2.1183 - acc: 0.2566 - val_loss: 2.0164 - val_acc: 0.2188\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.21875\n",
            "Epoch 6/100\n",
            "29936/29936 [==============================] - 36s 1ms/step - loss: 2.1072 - acc: 0.2614 - val_loss: 2.0047 - val_acc: 0.2188\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.21875\n",
            "Epoch 7/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 2.0913 - acc: 0.2665 - val_loss: 1.9749 - val_acc: 0.2344\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.21875 to 0.23438, saving model to /gdrive/My Drive/ted/ed-07-0.234375.hdf5\n",
            "Epoch 8/100\n",
            "29936/29936 [==============================] - 37s 1ms/step - loss: 2.0706 - acc: 0.2756 - val_loss: 1.9610 - val_acc: 0.2812\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.23438 to 0.28125, saving model to /gdrive/My Drive/ted/ed-08-0.281250.hdf5\n",
            "Epoch 9/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 2.0505 - acc: 0.2852 - val_loss: 1.9527 - val_acc: 0.2500\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.28125\n",
            "Epoch 10/100\n",
            "29936/29936 [==============================] - 36s 1ms/step - loss: 2.0339 - acc: 0.2919 - val_loss: 1.9449 - val_acc: 0.2812\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.28125\n",
            "Epoch 11/100\n",
            "29936/29936 [==============================] - 33s 1ms/step - loss: 2.0217 - acc: 0.3005 - val_loss: 1.9263 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.28125 to 0.34375, saving model to /gdrive/My Drive/ted/ed-11-0.343750.hdf5\n",
            "Epoch 12/100\n",
            "29936/29936 [==============================] - 37s 1ms/step - loss: 2.0115 - acc: 0.3040 - val_loss: 1.9312 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.34375\n",
            "Epoch 13/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 2.0031 - acc: 0.3095 - val_loss: 1.9232 - val_acc: 0.3281\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.34375\n",
            "Epoch 14/100\n",
            "29936/29936 [==============================] - 33s 1ms/step - loss: 1.9959 - acc: 0.3111 - val_loss: 1.8980 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.34375 to 0.35938, saving model to /gdrive/My Drive/ted/ed-14-0.359375.hdf5\n",
            "Epoch 15/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.9900 - acc: 0.3147 - val_loss: 1.9034 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.35938\n",
            "Epoch 16/100\n",
            "29936/29936 [==============================] - 36s 1ms/step - loss: 1.9849 - acc: 0.3167 - val_loss: 1.8890 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.35938\n",
            "Epoch 17/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.9803 - acc: 0.3200 - val_loss: 1.8882 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.35938\n",
            "Epoch 18/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.9764 - acc: 0.3187 - val_loss: 1.8988 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.35938 to 0.39062, saving model to /gdrive/My Drive/ted/ed-18-0.390625.hdf5\n",
            "Epoch 19/100\n",
            "29936/29936 [==============================] - 33s 1ms/step - loss: 1.9727 - acc: 0.3209 - val_loss: 1.8867 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.39062\n",
            "Epoch 20/100\n",
            "29936/29936 [==============================] - 23s 780us/step - loss: 1.9689 - acc: 0.3220 - val_loss: 1.8928 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.39062\n",
            "Epoch 21/100\n",
            "29936/29936 [==============================] - 27s 916us/step - loss: 1.9655 - acc: 0.3244 - val_loss: 1.8882 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.39062\n",
            "Epoch 22/100\n",
            "29936/29936 [==============================] - 25s 835us/step - loss: 1.9624 - acc: 0.3249 - val_loss: 1.8836 - val_acc: 0.3281\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.39062\n",
            "Epoch 23/100\n",
            "29936/29936 [==============================] - 28s 932us/step - loss: 1.9597 - acc: 0.3264 - val_loss: 1.8816 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.39062\n",
            "Epoch 24/100\n",
            "29936/29936 [==============================] - 33s 1ms/step - loss: 1.9569 - acc: 0.3290 - val_loss: 1.8698 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.39062\n",
            "Epoch 25/100\n",
            "29936/29936 [==============================] - 28s 927us/step - loss: 1.9539 - acc: 0.3297 - val_loss: 1.8687 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.39062\n",
            "Epoch 26/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.9515 - acc: 0.3296 - val_loss: 1.8757 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.39062\n",
            "Epoch 27/100\n",
            "29936/29936 [==============================] - 33s 1ms/step - loss: 1.9490 - acc: 0.3314 - val_loss: 1.8709 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.39062\n",
            "Epoch 28/100\n",
            "29936/29936 [==============================] - 33s 1ms/step - loss: 1.9460 - acc: 0.3329 - val_loss: 1.8705 - val_acc: 0.3281\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.39062\n",
            "Epoch 29/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.9432 - acc: 0.3334 - val_loss: 1.8561 - val_acc: 0.3281\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.39062\n",
            "Epoch 30/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.9409 - acc: 0.3344 - val_loss: 1.8642 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.39062\n",
            "Epoch 31/100\n",
            "29936/29936 [==============================] - 36s 1ms/step - loss: 1.9381 - acc: 0.3343 - val_loss: 1.8565 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.39062\n",
            "Epoch 32/100\n",
            "29936/29936 [==============================] - 32s 1ms/step - loss: 1.9358 - acc: 0.3357 - val_loss: 1.8523 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.39062\n",
            "Epoch 33/100\n",
            "29936/29936 [==============================] - 33s 1ms/step - loss: 1.9339 - acc: 0.3352 - val_loss: 1.8606 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.39062\n",
            "Epoch 34/100\n",
            "29936/29936 [==============================] - 37s 1ms/step - loss: 1.9307 - acc: 0.3381 - val_loss: 1.8526 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.39062\n",
            "Epoch 35/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.9291 - acc: 0.3383 - val_loss: 1.8473 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.39062 to 0.40625, saving model to /gdrive/My Drive/ted/ed-35-0.406250.hdf5\n",
            "Epoch 36/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.9268 - acc: 0.3387 - val_loss: 1.8516 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.40625\n",
            "Epoch 37/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.9247 - acc: 0.3409 - val_loss: 1.8419 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.40625\n",
            "Epoch 38/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.9228 - acc: 0.3417 - val_loss: 1.8505 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.40625\n",
            "Epoch 39/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.9197 - acc: 0.3425 - val_loss: 1.8498 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.40625\n",
            "Epoch 40/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.9185 - acc: 0.3427 - val_loss: 1.8574 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.40625\n",
            "Epoch 41/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.9168 - acc: 0.3428 - val_loss: 1.8479 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.40625\n",
            "Epoch 42/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.9151 - acc: 0.3437 - val_loss: 1.8387 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.40625\n",
            "Epoch 43/100\n",
            "29936/29936 [==============================] - 32s 1ms/step - loss: 1.9131 - acc: 0.3445 - val_loss: 1.8391 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.40625\n",
            "Epoch 44/100\n",
            "29936/29936 [==============================] - 29s 961us/step - loss: 1.9114 - acc: 0.3468 - val_loss: 1.8442 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.40625\n",
            "Epoch 45/100\n",
            "29936/29936 [==============================] - 31s 1ms/step - loss: 1.9096 - acc: 0.3468 - val_loss: 1.8417 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.40625\n",
            "Epoch 46/100\n",
            "29936/29936 [==============================] - 31s 1ms/step - loss: 1.9083 - acc: 0.3475 - val_loss: 1.8252 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.40625\n",
            "Epoch 47/100\n",
            "29936/29936 [==============================] - 31s 1ms/step - loss: 1.9070 - acc: 0.3479 - val_loss: 1.8259 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.40625\n",
            "Epoch 48/100\n",
            "29936/29936 [==============================] - 32s 1ms/step - loss: 1.9054 - acc: 0.3472 - val_loss: 1.8491 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.40625\n",
            "Epoch 49/100\n",
            "29936/29936 [==============================] - 33s 1ms/step - loss: 1.9041 - acc: 0.3485 - val_loss: 1.8338 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.40625\n",
            "Epoch 50/100\n",
            "29936/29936 [==============================] - 25s 845us/step - loss: 1.9027 - acc: 0.3492 - val_loss: 1.8641 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.40625\n",
            "Epoch 51/100\n",
            "29936/29936 [==============================] - 28s 923us/step - loss: 1.9013 - acc: 0.3493 - val_loss: 1.8356 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00051: val_acc improved from 0.40625 to 0.42188, saving model to /gdrive/My Drive/ted/ed-51-0.421875.hdf5\n",
            "Epoch 52/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.9005 - acc: 0.3497 - val_loss: 1.8331 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.42188\n",
            "Epoch 53/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.8987 - acc: 0.3497 - val_loss: 1.8240 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.42188\n",
            "Epoch 54/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.8979 - acc: 0.3509 - val_loss: 1.8224 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.42188\n",
            "Epoch 55/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.8966 - acc: 0.3492 - val_loss: 1.8217 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.42188\n",
            "Epoch 56/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.8950 - acc: 0.3511 - val_loss: 1.8162 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.42188\n",
            "Epoch 57/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.8941 - acc: 0.3514 - val_loss: 1.8150 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.42188\n",
            "Epoch 58/100\n",
            "29936/29936 [==============================] - 31s 1ms/step - loss: 1.8927 - acc: 0.3521 - val_loss: 1.8158 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.42188\n",
            "Epoch 59/100\n",
            "29936/29936 [==============================] - 22s 730us/step - loss: 1.8915 - acc: 0.3510 - val_loss: 1.8120 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.42188\n",
            "Epoch 60/100\n",
            "29936/29936 [==============================] - 25s 831us/step - loss: 1.8896 - acc: 0.3545 - val_loss: 1.8130 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.42188\n",
            "Epoch 61/100\n",
            "29936/29936 [==============================] - 15s 504us/step - loss: 1.8897 - acc: 0.3517 - val_loss: 1.8144 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.42188\n",
            "Epoch 62/100\n",
            "29936/29936 [==============================] - 15s 495us/step - loss: 1.8880 - acc: 0.3536 - val_loss: 1.8145 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.42188\n",
            "Epoch 63/100\n",
            "29936/29936 [==============================] - 22s 725us/step - loss: 1.8865 - acc: 0.3551 - val_loss: 1.8184 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.42188\n",
            "Epoch 64/100\n",
            "29936/29936 [==============================] - 32s 1ms/step - loss: 1.8857 - acc: 0.3547 - val_loss: 1.8180 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.42188\n",
            "Epoch 65/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.8844 - acc: 0.3546 - val_loss: 1.8155 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.42188\n",
            "Epoch 66/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.8835 - acc: 0.3547 - val_loss: 1.8173 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.42188\n",
            "Epoch 67/100\n",
            "29936/29936 [==============================] - 31s 1ms/step - loss: 1.8825 - acc: 0.3563 - val_loss: 1.8148 - val_acc: 0.3281\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.42188\n",
            "Epoch 68/100\n",
            "29936/29936 [==============================] - 30s 1ms/step - loss: 1.8816 - acc: 0.3546 - val_loss: 1.8107 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.42188\n",
            "Epoch 69/100\n",
            "29936/29936 [==============================] - 32s 1ms/step - loss: 1.8803 - acc: 0.3568 - val_loss: 1.8283 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.42188\n",
            "Epoch 70/100\n",
            "29936/29936 [==============================] - 33s 1ms/step - loss: 1.8784 - acc: 0.3573 - val_loss: 1.8135 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.42188\n",
            "Epoch 71/100\n",
            "29936/29936 [==============================] - 29s 965us/step - loss: 1.8787 - acc: 0.3578 - val_loss: 1.8131 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.42188\n",
            "Epoch 72/100\n",
            "29936/29936 [==============================] - 30s 993us/step - loss: 1.8768 - acc: 0.3566 - val_loss: 1.8096 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.42188\n",
            "Epoch 73/100\n",
            "29936/29936 [==============================] - 38s 1ms/step - loss: 1.8762 - acc: 0.3579 - val_loss: 1.8236 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.42188\n",
            "Epoch 74/100\n",
            "29936/29936 [==============================] - 31s 1ms/step - loss: 1.8745 - acc: 0.3584 - val_loss: 1.8058 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.42188\n",
            "Epoch 75/100\n",
            "29936/29936 [==============================] - 31s 1ms/step - loss: 1.8735 - acc: 0.3580 - val_loss: 1.8348 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.42188\n",
            "Epoch 76/100\n",
            "29936/29936 [==============================] - 33s 1ms/step - loss: 1.8731 - acc: 0.3583 - val_loss: 1.8194 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.42188\n",
            "Epoch 77/100\n",
            "29936/29936 [==============================] - 30s 1ms/step - loss: 1.8716 - acc: 0.3602 - val_loss: 1.8199 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.42188\n",
            "Epoch 78/100\n",
            "29936/29936 [==============================] - 32s 1ms/step - loss: 1.8708 - acc: 0.3586 - val_loss: 1.8250 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.42188\n",
            "Epoch 79/100\n",
            "29936/29936 [==============================] - 31s 1ms/step - loss: 1.8701 - acc: 0.3595 - val_loss: 1.8055 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.42188\n",
            "Epoch 80/100\n",
            "29936/29936 [==============================] - 27s 893us/step - loss: 1.8688 - acc: 0.3609 - val_loss: 1.8154 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.42188\n",
            "Epoch 81/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.8679 - acc: 0.3595 - val_loss: 1.8226 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.42188\n",
            "Epoch 82/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.8668 - acc: 0.3595 - val_loss: 1.8160 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.42188\n",
            "Epoch 83/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.8658 - acc: 0.3612 - val_loss: 1.8212 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.42188\n",
            "Epoch 84/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.8646 - acc: 0.3614 - val_loss: 1.8192 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.42188\n",
            "Epoch 85/100\n",
            "29936/29936 [==============================] - 32s 1ms/step - loss: 1.8639 - acc: 0.3610 - val_loss: 1.8044 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.42188\n",
            "Epoch 86/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.8626 - acc: 0.3626 - val_loss: 1.8125 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.42188\n",
            "Epoch 87/100\n",
            "29936/29936 [==============================] - 36s 1ms/step - loss: 1.8619 - acc: 0.3617 - val_loss: 1.8118 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.42188\n",
            "Epoch 88/100\n",
            "29936/29936 [==============================] - 32s 1ms/step - loss: 1.8597 - acc: 0.3622 - val_loss: 1.8151 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.42188\n",
            "Epoch 89/100\n",
            "29936/29936 [==============================] - 32s 1ms/step - loss: 1.8594 - acc: 0.3635 - val_loss: 1.8161 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.42188\n",
            "Epoch 90/100\n",
            "29936/29936 [==============================] - 36s 1ms/step - loss: 1.8585 - acc: 0.3618 - val_loss: 1.8024 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.42188\n",
            "Epoch 91/100\n",
            "29936/29936 [==============================] - 35s 1ms/step - loss: 1.8583 - acc: 0.3629 - val_loss: 1.8230 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.42188\n",
            "Epoch 92/100\n",
            "29936/29936 [==============================] - 30s 997us/step - loss: 1.8566 - acc: 0.3636 - val_loss: 1.8037 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.42188\n",
            "Epoch 93/100\n",
            "29936/29936 [==============================] - 25s 831us/step - loss: 1.8554 - acc: 0.3638 - val_loss: 1.8103 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.42188\n",
            "Epoch 94/100\n",
            "29936/29936 [==============================] - 33s 1ms/step - loss: 1.8550 - acc: 0.3657 - val_loss: 1.7958 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.42188\n",
            "Epoch 95/100\n",
            "29936/29936 [==============================] - 27s 909us/step - loss: 1.8532 - acc: 0.3650 - val_loss: 1.8294 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00095: val_acc improved from 0.42188 to 0.43750, saving model to /gdrive/My Drive/ted/ed-95-0.437500.hdf5\n",
            "Epoch 96/100\n",
            "29936/29936 [==============================] - 24s 818us/step - loss: 1.8526 - acc: 0.3644 - val_loss: 1.8073 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.43750\n",
            "Epoch 97/100\n",
            "29936/29936 [==============================] - 26s 856us/step - loss: 1.8512 - acc: 0.3647 - val_loss: 1.8171 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.43750\n",
            "Epoch 98/100\n",
            "29936/29936 [==============================] - 26s 860us/step - loss: 1.8501 - acc: 0.3680 - val_loss: 1.8105 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.43750\n",
            "Epoch 99/100\n",
            "29936/29936 [==============================] - 34s 1ms/step - loss: 1.8500 - acc: 0.3677 - val_loss: 1.8079 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.43750\n",
            "Epoch 100/100\n",
            "29936/29936 [==============================] - 33s 1ms/step - loss: 1.8485 - acc: 0.3678 - val_loss: 1.7942 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.43750\n",
            "[+] stop time for training >>> 2019-07-17 21:00:19\n",
            "[+] Training Accuracy : 0.368333\n",
            "[+] Training Loss     : 1.843871\n",
            "[+] Testing Accuracy  : 0.342000\n",
            "[+] Testing Loss      : 1.843871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAFACAYAAACoSyokAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QFdfk3b9FC_",
        "colab_type": "text"
      },
      "source": [
        "# **GETTING EMOTIONS OF OUR TEXT**\n",
        "*`predicting using our model`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7Bi2Ffu9EwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "85584222-5e82-49be-d550-fc738b15fb99"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "import numpy as np, emojis, nltk\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "drive.mount('/gdrive')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "pickle_f = open(\"/gdrive/My Drive/ted/word_index.pkl\",\"rb\")\n",
        "vocab = pickle.load(pickle_f)\n",
        "\n",
        "\n",
        "def getEmojis(i):\n",
        "    \n",
        "    emojies = {\n",
        "         0: ':angry:', 1: ':expressionless:', 2: ':no_mouth:', 3: ':heart_eyes:', \n",
        "         4: ':smile:', 5: ':blush:', 6: ':-1:', 7: ':heart:' , 8: ':neutral_face:',\n",
        "         9: ':sweat_smile:', 10: ':pensive:', 11: ':anguished:' , 12: ':worried:'\n",
        "        }\n",
        "    \n",
        "    return emojies[i]\n",
        "\n",
        "\n",
        "def textToInput(x):\n",
        "    '''\n",
        "    fit_on_texts Updates internal vocabulary based on \n",
        "    a list of texts. This method creates the vocabulary \n",
        "    index based on word frequency. So if you give it something like, \n",
        "    \"The cat sat on the mat.\" It will create a dictionary \n",
        "    s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 \n",
        "    it is word -> index dictionary so every word gets \n",
        "    a unique integer value. 0 is reserved for padding. \n",
        "    So lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
        "    \n",
        "    texts_to_sequences Transforms each text in texts to \n",
        "    a sequence of integers. So it basically takes each \n",
        "    word in the text and replaces it with its corresponding \n",
        "    integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
        "    Why don't combine them? Because you almost always fit once \n",
        "    and convert to sequences many times. You will fit on your \n",
        "    training corpus once and use that exact same  word_index \n",
        "    dictionary at train / eval / testing / prediction time to \n",
        "    convert actual text into sequences to feed them to the network. \n",
        "    So it makes sense to keep those methods separate.\n",
        "    '''\n",
        "    \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    new_stop_words = set(stop_words)\n",
        "    for s in stop_words:\n",
        "        new_stop_words.add(s.replace('\\'',''))\n",
        "        pass\n",
        "    stop_words = new_stop_words\n",
        "    base_filters = '\\n\\t!\"#$%&()*+,-./:;<=>?[\\]^_`{|}~ '\n",
        "    word_sequences = []\n",
        "    for i in x:\n",
        "        i = str(i)\n",
        "        i = i.replace('\\'', '')\n",
        "        newlist = [x for x in text_to_word_sequence(i,filters = base_filters, lower = True) if not x.startswith(\"@\")]\n",
        "        filtered_sentence = [w for w in newlist if not w in stop_words] \n",
        "        word_sequences.append(filtered_sentence)\n",
        "        pass\n",
        "   \n",
        "    word_indices = []\n",
        "    for seq in word_sequences:\n",
        "        word_indices.append([vocab[x] for x in seq if x in vocab.keys()])\n",
        "        \n",
        "    print(\"\\n[+] word indices before padding ... \")\n",
        "    print(word_indices)\n",
        "    \n",
        "    x_data = pad_sequences(word_indices, maxlen=20)\n",
        "    print(\"\\n[+] NN input is <<< word indices , 20 length of each sequence after padded >>>: \")\n",
        "    print(x_data)\n",
        "    print()\n",
        "    return x_data\n",
        "\n",
        "\n",
        "\n",
        "# all these words must be in our vocab which is built based on our dataset\n",
        "# in this case we have 32855 words in our entire dataset , any word that \n",
        "# doesn't exit in there won't have vector(cause the embedding layer has all vector of our vocab)! \n",
        "# cause we can't find its index in our vocab.\n",
        "text = [\"can you see the sky, there is a little hole in there!\", \n",
        "        \"unknown sense your toy inside her room before his mother come into me\",\n",
        "        \"the red sky just hit me and my face is on fire right now\"\n",
        "       ]\n",
        "\n",
        "text_mat = np.asarray(text)\n",
        "NNInput = textToInput(text_mat)\n",
        "model = load_model('/gdrive/My Drive/ted/ted.hdf5')\n",
        "prediction = model.predict(NNInput)\n",
        "\n",
        "\n",
        "# getting the vector of a word according to its index in word_index(vocab) pickle\n",
        "embeddings = model.layers[0].get_weights()[0]\n",
        "words_embeddings = {w:embeddings[idx] for w, idx in vocab.items()}\n",
        "print(\"vector of love according to its index in our vocab is : \\n\", words_embeddings['love'])\n",
        "\n",
        "\n",
        "# -----------\n",
        "# prediction:\n",
        "# -----------\n",
        "# we'll choose the maximum prob to show the emotion of our text\n",
        "# we can also show every prob to the user in order to analysis his/her text!\n",
        "\n",
        "# [[0.00285324 0.00443429 0.02685553 0.02307291 0.04944    0.15052915\n",
        "#   0.01866804 0.06597449 0.2738337  0.04779249 0.10498895 0.06976512\n",
        "#   0.1617921 ]\n",
        "#  [0.0013604  0.00217447 0.00926796 0.01718927 0.0403096  0.09397643\n",
        "#   0.00903581 0.27401027 0.16561657 0.04202646 0.10514135 0.0530719\n",
        "#   0.18681952]]\n",
        "\n",
        "for i in prediction:\n",
        "    print(emojis.encode(f' you are  {getEmojis(np.argmax(i))}'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "[+] word indices before padding ... \n",
            "[[24, 1259, 92, 2109], [5932, 1252, 1761, 665, 323, 450, 72], [571, 1259, 370, 413, 1309, 58]]\n",
            "\n",
            "[+] NN input is <<< word indices , 20 length of each sequence after padded >>>: \n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0   24 1259   92 2109]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0 5932\n",
            "  1252 1761  665  323  450   72]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "   571 1259  370  413 1309   58]]\n",
            "\n",
            "vector of love according to its index in our vocab is : \n",
            " [-0.13886    1.1401    -0.85212   -0.29212    0.75534    0.82762\n",
            " -0.3181     0.0072204 -0.34762    1.0731    -0.24665    0.97765\n",
            " -0.55835   -0.090318   0.83182   -0.33317    0.22648    0.30913\n",
            "  0.026929  -0.086739  -0.14703    1.3543     0.53695    0.43735\n",
            "  1.2749    -1.4382    -1.2815    -0.15196    1.0506    -0.93644\n",
            "  2.7561     0.58967   -0.29473    0.27574   -0.32928   -0.201\n",
            " -0.28547   -0.45987   -0.14603   -0.69372    0.070761  -0.19326\n",
            " -0.1855    -0.16095    0.24268    0.20784    0.030924  -1.3711\n",
            " -0.28606    0.2898   ]\n",
            " you are  ðŸ˜\n",
            " you are  ðŸ˜\n",
            " you are  ðŸ˜Ÿ\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}