# -*- coding: utf-8 -*-
"""ClassificationOnText_EmotionDetection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1COiFNR5_tkZhjKYp_BHD8J74QH1GSPq6

***Powered by:***

![uniXerr logo](https://drive.google.com/uc?id=1TXJwfJsTJzU2M7LrIQgx2Tx4cfUzcQuX)

# **MLP STRUCTURE**

![mlp](https://drive.google.com/uc?id=1jeIYTAkZNMB31a5NalFhNSKMmx8-kBe-)

# **FEED FORWARD**
As we already know that in the feed froward we compute `A` and `dZ` such as
`Z = X.W`
`A = activation(Z) and dZ = dA = activation’(Z)`.
So to compute the dimensions of `A` and `dZ`, it suffices to look at the dimensions of `X` and `W`, which are `(m, 2)` and `(2, 4)` respectively.
The result is `(m, 2) x (2, 4) = (m, 4)`.
The following pictures depicts how the `A` and dZ are filled to form matrices of dimension `(m, 4)`.

![feed forward](https://drive.google.com/uc?id=1mMKFqLtgNjoR-g8MZoGhDBIUTNwS8jjM)

# **GRADIENT DESCENT ALGORITHM**

![gda](https://drive.google.com/uc?id=17qkqt03Iv5gz1iWLHdNA4TuF8dGG7pFt)

# **BACK PROPAGATION**
As a reminder, the formulas in the Back Propagation are:

`𝚫ᴸ = (Aᴸ - Y) * dZᴸ`

`𝚫ⁱ = (𝚫ⁱ⁺¹ . W ᵀ)* dZⁱ`

(where * is a member wise multiplication)
We know by now that both `A` and `dZ` have the dimensions `(m, n)` where m is the number of samples and n is the number of nodes in the layer. So the `𝚫ᴸ` will have the same dimension `(m, n)`
In our example the last layer (L) has only one node, so `A` and `dZ` at layer L have the dimension `(m, 1)`. It follows that `𝚫ᴸ` has also the dimension `(m, 1)`.
In order to compute the deltas of the inner layers, let’s take layers 2 and 3 of the example.

![bp](https://drive.google.com/uc?id=1t6AP_RnQHVab8H_iRh1PKlfYTw4P8kiP)

The weight `𝞱` between layer 2 and layer 3 has the dimension `(4, 3)`, since the 4 nodes of layer 2 are connected to 3 nodes of layers 3.

![theta](https://drive.google.com/uc?id=1jxXV5sWLH_-uKMiAS4DVBlhKt_XC3-kk)

However since the delta at layer 2 is the dot product of the delta coming from layer 3 and the weight `𝞱`, we end up with the following configuration.

![bp mat](https://drive.google.com/uc?id=1hK3tOL3VT5M32RSkHb4Il1LJHirXFgFj)

It not possible to do the dot product and obtain delta at layer 2 with dimension `(m, 4)`!
To solve this, we use `𝞱ᵀ` the transpose of `𝞱`.
The computation will be possible as shown by the images below

![bp matt](https://drive.google.com/uc?id=1aINCIzkYNtorAngdp4epCxT2vIJu3U0l)

Updating the weights at layer i, involves getting the input at the layer, perform the dot product with the delta of the layer then use the result to update the weights.
If we take layers 1 and 2 of our example, we have an input, at layer 1, with dimension `(m, 2)` and the dimension of delta at layer 2 `(m, 4)`.
However the weights matrix connecting layer 1 and 2 has the dimension
`(2, 4)`.

![bp mat](https://drive.google.com/uc?id=1DPImpBwwKEhZAOLkHrImu60vZygbj8xw)

To be able to update the weights matrix, the dot product between `X` and `𝚫` should result in a `(2, 3)` matrix.
This is done by getting the transpose of `X` that is `Xᵀ`, then perform the dot product `Xᵀ` . `𝚫` that we call `𝞭`.

![bp full mat comp](https://drive.google.com/uc?id=1ILL_wpGOHo-NKFCD7bsUEWJ3_YYKFbDH)

Now we can update the weight matrix using the gradient descent formula
`Wⁿ⁺¹ = Wⁿ - 𝝰 * 𝞭` (in here n is the version of W and not the layer index).

*Credit:* [Ziad SALLOUM](https://towardsdatascience.com/@zsalloum?source=post_page-----cc1de33e8397----------------------)

# **CONFUSION MATRIX**

In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabelling one as another).

![confusion visualization](https://drive.google.com/uc?id=1eVNJniz_QXc3MsPerNlz0XZEgK9DnE8C)

**Accuracy** — Accuracy refers to the closeness of a measured value to a standard or known value.

**Precision** — Precision refers to the closeness of two or more measurements to each other. It is the repeatability or reproducibility of the measurement.

**Recall(Sensitivity)** — Recall refers to the fraction of relevant instances that have been retrieved over the total amount of relevant instances.

# **BATCHES , EPOCH AND ITERATION**

In Deep Learning, an epoch is a hyperparameter which is defined before training a model. One epoch is when an entire dataset is passed both forward and backward through the neural network only once.

One epoch is too big to feed to the computer at once. So, we divide it in several smaller batches. We use more than one epoch because passing the entire dataset through a neural network is not enough and we need to pass the full dataset multiple times to the same neural network. But since we are using a limited dataset and to optimise the learning and the graph we are using Gradient Descent which is an iterative process. So, updating the weights with single pass or one epoch is not enough.

A batch is the total number of training examples present in a single batch and an iteration is the number of batches needed to complete one epoch.

For example: If we divide a dataset of 2000 training examples into 500 batches, then 4 iterations will complete 1 epoch.
The weights will be updated at the end of each training epoch.

![epoch_batches_training](https://qph.fs.quoracdn.net/main-qimg-5e4fa1bbf4c0138642d0d8bd298ef09f)



# **LSTM INPUT SHAPE**
[LSTM explained and its diagrams](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)

![lsmt_d1](https://miro.medium.com/max/700/1*laH0_xXEkFE0lKJu54gkFQ.png)

*X0 , X1 , .... , X30 is timesteps which in our case is 20 inputs (word) which every Xi has a length of 50 and our lstm has 100 layers with none batches (cause we don't have rows in our data and the data is just a vector with size 50 for each input (word) ).
20 is the time steps per sample which means every sample has 20 rows ; every text is padded by 20 (20 words maximum) and is the length of each sequence(sentence) in each sample ; so for every sample we have a (20 * 50) matrix full of a vector(features) with size 50 for each word. in total we have a tensor with size (30000*20*50). 
*

```
N=1
model = Sequential()
model.add(LSTM(N))
```

![types of lstm](https://i.stack.imgur.com/xLZCK.png)
*For the other models you would need N>1*
![types of lstm](https://i.stack.imgur.com/PQs02.png)



```
<--- Many To One With Multiple Features [input_shape = (20, 50)] --->

[ -> size = 30000 = 30000 samples
    [ -> size = 20 = 20 rows = time_steps
        [........] -> size = 50 = 50 features
        [........]
            .
            .
            .
        [........]
        [........]
    ]

    [ -> size = 20 = 20 rows = time_steps
        [........] -> size = 50 = 50 features
        [........]
            .
            .
            .
        [........]
        [........]
    ]

    .
    .
    .

    [ -> size = 20 = 20 rows = time_steps
        [........] -> size = 50 = 50 features
        [........]
            .
            .
            .
        [........]
        [........]
    ]
]
```

[LSTM diagrams and its text classification example](https://adventuresinmachinelearning.com/keras-lstm-tutorial/)

![lstm_d2](https://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-architecture.png)


*The input of the LSTM is always is a 3D array. (batch_size, time_steps, seq_len)
The output of the LSTM could be a 2D array or 3D array depending upon the return_sequences argument.
If return_sequence is False, the output is a 2D array. (batch_size, units)
If return_sequence is True, the output is a 3D array. (batch_size, time_steps, units)
You always have to give a three-dimensional array as an input to your LSTM network (refer to the above image). Where the first dimension represents the batch size, the second dimension represents the number of time-steps you are feeding a sequence. And the third dimension represents the number of units in one input sequence. For example, input shape looks like (batch_size, time_steps, seq_len).*

[LSTM input & output shape](https://medium.com/@shivajbd/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e)

![lstm_d3](https://miro.medium.com/max/532/1*v5_QpzkQfufVogeCY9eaOw.png)


# **What is the Difference Between a 1D CNN and a 2D CNN?**
CNNs work the same way whether they have 1, 2, or 3 dimensions. The difference is the structure of the input data and how the filter, also known as a convolution kernel or feature detector, moves across the data.
![conv1D](https://missinglink.ai/wp-content/uploads/2019/03/1D-convolutional-example_2x.png)

*In this natural language processing (NLP) example, a sentence is made up of 9 words. Each word is a vector that represents a word. The filter covers at least one word; a height parameter specifies how many words the filter should consider at once. In this example the height is 2, meaning the filter moves 8 times to fully scan the data.*

![conv2D](https://missinglink.ai/wp-content/uploads/2019/03/2D-convolutional-example_2x-700x584.png)

*In a 2D convolutional network, each pixel within the image is represented by its x and y position as well as the depth, representing image channels (red, green, and blue). The filter in this example is 2×2 pixels. It moves over the images both horizontally and vertically.
Another difference between 1D and 2D networks is that 1D networks allow you to use larger filter sizes. In a 1D network, a filter of size 7 or 9 contains only 7 or 9 feature vectors. Whereas in a 2D CNN, a filter of size 7 will contain 49 feature vectors, making it a very broad selection.
Another difference, though, is the fact that you can afford to use larger convolution windows with 1D CNNs. With a 2D convolution layer, a 3 × 3 convolution window contains 3 × 3 = 9 feature vectors. With 1D convolution layer, a window of size 3 contains only 3 feature vectors. You can thus easily afford 1D convolution windows of size 7 or 9.*



---

# **BUILDING MODEL**
![embedding_matrix](https://miro.medium.com/max/700/1*zaXqnINwmKfac2lrNqjTVw.png)


![model](http://www.upsara.com/images/z50950_ted_NN_model.png)

---

# **TRAINING , VALID AND TESTING DATA**
If you want to build a solid model you have to follow that specific protocol of 
splitting your data into three sets: One for training, 
one for validation and one for final evalution, which is the test set.
The idea is that you train on your training data and tune your 
model with the results of metrics (accuracy, loss etc) that you get from your validation set.
Your model doesn't "see" your validation set and isn´t in any way trained on it, 
but you as the architect and master of the hyperparameters tune the model according to this data. 
Therefore it indirectly influences your model because it directly influences 
your design decisions. You nudge your model to work well with the validation 
data and that can possibly bring in a tilt.
Exactly that is the reason you only evaluate your models final score on data 
that neither your model nor you yourself has used – and that is the third chunk of data, your test set.
Only this procedure makes sure you get an unaffected view of your models quality
and ability to generalize what is has learned on totally unseen data.


# **Overfitting**
Overfitting occurs when a statistical model or machine learning algorithm captures the noise of the data.  
Intuitively, overfitting occurs when the model or the algorithm fits the data too well.  
Specifically, overfitting occurs if the model or algorithm shows low bias but high variance.  
Overfitting is often a result of an excessively complicated model, and it can be prevented by 
fitting multiple models and using validation or cross-validation to compare their predictive accuracies on test data.
When training a machine learning model, one of the main things that 
you want to avoid would be overfitting. This is when your model fits 
the training data well, but it isn't able to generalize and make 
accurate predictions for data it hasn't seen before.
To find out if their model is overfitting, data scientists 
use a technique called cross-validation, where they split their 
data into two parts - the training set, and the validation set. 
The training set is used to train the model, while the validation set 
is only used to evaluate the model's performance.
Metrics on the training set let you see how your model is progressing in 
terms of it's training, but it's metrics on the validation set that let 
you get a measure of the quality of your model - how well it's able to 
make new predictions based on data it hasn't seen before.
With this in mind, loss and acc are measures of loss and accuracy on 
the training set, while val_loss and val_acc are measures of loss and accuracy 
on the validation set. At the moment the model has an accuracy of ~38% on the 
training set and ~40% on the validation set. This means that you can expect your 
model to perform with ~40% accuracy on new data.
as the epochs goes from 10 to 100, your acc metric increases, 
while your val_acc metric decreases. This means that your model is fitting the 
training set better, but is losing it's ability to predict on new data, 
indicating that your model is starting to fit on noise and is beginning to overfit.


# **Underfitting**
Underfitting occurs when a statistical model or machine learning algorithm cannot 
capture the underlying trend of the data.  Intuitively, underfitting occurs when 
the model or the algorithm does not fit the data well enough. Specifically, 
underfitting occurs if the model or algorithm shows low variance but high bias.  
Underfitting is often a result of an excessively simple model.


# **Tricks**
If your training loss is much lower than validation loss then 
this means the network might be overfitting. Solutions to this 
are to decrease your network size, or to increase dropout. 
For example you could try dropout of 0.5 and so on.
If your training/validation loss are about equal then your model 
is underfitting. Increase the size of your model 
(either number of layers or the raw number of neurons per layer)
a typical reason for validation accuracy being lower than 
training accuracy was overfitting. when the opposite is true 
it’s because the model is underfitting the data.


 
```
.
.
.
.

# Epoch 100/100
# 29936/29936 [==============================] - 10s 326us/step - loss: 1.8454 - acc: 0.3736 - val_loss: 1.7907 - val_acc: 0.3906
```



**in all epochs we have underfitting , due to this problem our  model comes into underfitting area , cause we don't have enough dataset!**

# **TEXT CLASSIFICATION - EMOTION DETECTION KERAS CODE**
*`getting our hands on`*
"""

# -*- encoding: utf-8 -*-


import numpy as np, pandas as pd, os, sys, time, datetime, nltk
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from keras.models import Sequential, load_model, save_model, Model
from keras.layers import Dense, Dropout, Activation, Embedding, Flatten, LSTM, Bidirectional, Embedding, Flatten, Conv1D, MaxPooling1D
from keras.preprocessing.text import text_to_word_sequence, Tokenizer
from nltk.corpus import stopwords
from keras.preprocessing.sequence import pad_sequences
from keras.utils import np_utils
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import ModelCheckpoint
from google.colab import drive
import pickle
drive.mount('/gdrive')
nltk.download('stopwords')
plt.style.use('ggplot')


# ===================================
# Preprocessing and formating
# ===================================

MAX_SEQUENCE_LENGTH = 20
EMBEDDING_DIM = 50
dataFrame = pd.read_csv('/gdrive/My Drive/text_emotion_twitter.csv', encoding='utf-8')
# dataFrame is a matrix of 40000 rows X 4 cols
# dataFrame.values[[rows], [cols]]
x = dataFrame.values[:,3] # accessing third elements of all rows which is content/text => 40000 texts/contents
y = dataFrame.values[:,1] # accessing first elements of all rows which is label/emotions => 40000 labels/emotions
print("[+] data size from csv >>> ", x.shape)
print("[+] label size from csv >>> ", y.shape)


def uprint(*objects, sep=' ', end='\n', file=sys.stdout):
    enc = file.encoding
    if enc == 'UTF-8':
        print(*objects, sep=sep, end=end, file=file)
    else:
        f = lambda obj: str(obj).encode(enc, errors='backslashreplace').decode(enc)
        print(*map(f, objects), sep=sep, end=end, file=file)


def plot_history(history):
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize = (12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label = 'Training acc')
    plt.plot(x, val_acc, 'r', label = 'Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label = 'Training loss')
    plt.plot(x, val_loss, 'r', label = 'Validation loss')
    plt.title('Training and validation loss')
    plt.legend()


stop_words = set(stopwords.words('english'))
new_stop_words = set(stop_words)
# adding woudlnt type of words into stopwords list
for s in stop_words:
    new_stop_words.add(s.replace('\'',''))
    pass
stop_words = new_stop_words
print("\n[+] Excluding stopwords ...")


# removing @ from default base filter, to remove that whole word, which might be considered as user or page name
base_filters = '\n\t!"#$%&()*+,-./:;<=>?[\]^_`{|}~ '
word_sequences = []
for i in x:
    i = str(i)
#     uprint(i)
    i = i.replace('\'', '')
    newlist = [x for x in text_to_word_sequence(i,filters = base_filters, lower = True) if not x.startswith("@")]
    filtered_sentence = [w for w in newlist if not w in stop_words] 
    word_sequences.append(filtered_sentence)
    pass
# print("\n[+] word sequences >>> ", word_sequences) # len is : 40000
# [ ['know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part'], ....... , ['layin', 'n', 'bed', 'headache', 'ughhhh', 'waitin', 'call'] ]



# Tokenizing words to word indices
tokenizer = Tokenizer()
# fitting texts to tokenizer object
tokenizer.fit_on_texts(word_sequences)
word_indices = tokenizer.texts_to_sequences(word_sequences)
word_index = tokenizer.word_index # size is 32855 cause its a dict with key as word and index as value
print("\n[+] Saving word_index into picke.... ")
with open("/gdrive/My Drive/ted/word_index.pkl","wb") as f:
    pickle.dump(word_index, f)
# print("\n[+] word index >>> ", word_index) 
# word_index : { 'im': 1, 'day': 2, 'good': 3, 'get': 4, 'like': 5, ......  , 'quot': 6, 'http': 7, 'go': 8, 'today': 9 }
print("\n[+] Tokenized to Word indices as ")
print(np.array(word_indices).shape) # size is 40000
# print("\n[+] word indices >>> ", word_indices)
# word_indices : [ [20, 3077, 57, 4396, 714, 489, 1014, 433], ...... , [8964, 177, 74, 319, 3327, 2077, 192], [2555, 3963, 1598, 143] ]



# padding word_indices
# >>> pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]])
# array([[0, 1, 2, 3],
#        [3, 4, 5, 6],
#        [0, 0, 7, 8]], dtype=int32)

x_data = pad_sequences(word_indices, maxlen=MAX_SEQUENCE_LENGTH)
print("\n[+] After padding data size is")
print(x_data.shape) # (40000, 20)
print("\n[+] padded x_data >>> ")
print(x_data)
# [[    0     0     0 ...   489  1014   433]
#  [    0     0     0 ...  3327  2077   192]
#  [    0     0     0 ...  3963  1598   143]
#  ...
#  [    0     0     0 ...  1390   165     2]
#  [    0     0     0 ...  1135    19   336]
#  [    0     0     0 ... 32852 32853 32854]]




# ===================================
# Building Embedding Layer
# ===================================


# using pretrained glove vector
print("\n[+] Loading Glove Vectors ...")
embeddings_index = {}
f = open(os.path.join('', '/gdrive/My Drive/glove.6B.50d.txt'),'r',encoding="utf-8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('\n[+] Loaded GloVe Vectors Successfully!')


# embedding_matrix size is : vocab + 1 X 50
embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

print("\n[+] Embedding Matrix Generated with size >>>> ", embedding_matrix.shape)
print("\n[+] embedding matrix is ")
print(embedding_matrix)
# it has len(word_index) + 1 rows and 50 cols which is (32855, 50)
# [[ 0.          0.          0.         ...  0.          0.
#    0.        ]
#  [-0.067678    0.51832002  1.32599998 ... -0.65103     0.12924001
#    0.48723999]
#  [ 0.11626     0.53896999 -0.39513999 ... -0.39061999 -0.10885
#    0.084513  ]
#  ...
#  [ 1.18879998  1.46720004 -0.99624002 ...  0.48704001  0.77978998
#    0.38242999]
#  [ 0.013849   -0.54549998 -0.077683   ...  0.67878002  0.46202999
#    0.72376001]
#  [ 0.023778   -0.80124003  0.1193     ... -1.06830001 -0.35703
#    0.21013001]]




"""
This layer acts as lookup table for vectors, given word index. 
It will return embedded word vector.
Embedding layer can only be used as first layer in Keras.
Our input layer will of be size : (None,20) ; None means variable number.
As we have padded 20 words for each input, in data preparation stage. 
we have 20 word indices in each row.
Output of Embedding layer will be fed to this LSTM layer.

WATNING : we need very large dataset to get a vector for our word , because of embedding layer manner and tokenizer algorithm
          we can only get the vector of those words which exists in our dataset and anything else will fall into fail
          cause our new word doesn't exit in our word_index(vocab) so it won't give us its index and all we get is 0 vector or none!
"""
embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)


# ===================================
# One Hot encoding Labels
# ===================================


# Example from sklearn API :

# >>> le = preprocessing.LabelEncoder()
# >>> le.fit(["paris", "paris", "tokyo", "amsterdam"])
# LabelEncoder()
# >>> list(le.classes_)
# ['amsterdam', 'paris', 'tokyo']
# >>> le.transform(["tokyo", "tokyo", "paris"]) 
# array([2, 2, 1]...)
# >>> list(le.inverse_transform([2, 2, 1]))
# ['tokyo', 'tokyo', 'paris']


label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(y) # Fit label encoder and return encoded labels
# print(label_encoder.classes_)
# print(label_encoder.transform(label_encoder.classes_)) 

le_name_mapping = dict(zip(label_encoder.transform(label_encoder.classes_),label_encoder.classes_))
print("\n[+] Label Encoding Classes as ")
print(le_name_mapping)
# {0: 'anger', 1: 'boredom', 2: 'empty', 3: 'enthusiasm', 4: 'fun', 5: 'happiness', 6: 'hate', 7: 'love', 8: 'neutral', 9: 'relief', 10: 'sadness', 11: 'surprise', 12: 'worry'}



"""
to_categorical

Converts a class vector (integers) to binary class matrix.

Arguments

y: class vector to be converted into a matrix (integers from 0 to num_classes).
num_classes: total number of classes.
dtype: The data type expected by the input, as a string (float32, float64, int32...)

Return

A binary matrix representation of the input. The classes axis is placed last.

Example

# Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:
> labels
array([0, 2, 1, 2, 0])
# `to_categorical` converts this into a matrix with as many
# columns as there are classes. The number of rows
# stays the same.
> to_categorical(labels)
array([[ 1.,  0.,  0.],
       [ 0.,  0.,  1.],
       [ 0.,  1.,  0.],
       [ 0.,  0.,  1.],
       [ 1.,  0.,  0.]], dtype=float32)
       
"""

y_data = np_utils.to_categorical(integer_encoded)
print("\n[+] One Hot Encoded class shape with size ")
print(y_data.shape) # (40000 , 13)
print("\n[+] One hot encoded labels matrix")
print(y_data)
# [[0. 0. 1. ... 0. 0. 0.]
#  [0. 0. 0. ... 1. 0. 0.]
#  [0. 0. 0. ... 1. 0. 0.]
#  ...
#  [0. 0. 0. ... 0. 0. 0.]
#  [0. 0. 0. ... 0. 0. 0.]
#  [0. 0. 0. ... 0. 0. 0.]]




# ===================================
# Building Model
# ===================================


"""
We’ll use LSTM layer with 100 units. This layer has 100 RNN Cells, 
this number is variable and can be adjusted according to our need and complexity of our data.
Input given to LSTM will be considered as (batch_size, timesteps, features).

samples/batch_size. These are the rows in your data : is None , cause our data is a vector with size 50 in each input(timestep)
timesteps. These are the past observations for a feature, such as lag variables. : 20 is the number of input cause we padded each word with 20 length , 
                                                                                   20 words(timesteps) 50D(feature) vector with none rows(batch_size) for each text
features. These are columns in your data : 50 , as the dim of each word in each input

-----------------------
return_sequences=True :
-----------------------
Output of RNN layer will include all the outputs from all the units/cells in that layer.
(None, 20,50) => LSTM(100, return_sequences=True) => (None, 20, 100)

[NOTE] : batch_size is None cause we don't have rows in our data (our data is a vector with size 50 for each input(timestep))

In the next step, we’ll flatten.
(None, 20,100) => Flatten => (None,2000)
If you wish to connect a Dense layer directly to an Embedding layer, 
you must first flatten the 2D output matrix to a 1D vector using the Flatten layer.

-----------------------------------------
Understanding Dense Layer (Last Layers) :
-----------------------------------------
We connect all the data that we get from previous levels using Dense Layers.. 
We keep reducing output units to (None, labels_count) by adding multiple Dense Layers.
(None, 2000) => Dense(300) => (None,300)
Adding another dense layer
(None,300) => Dense(labels_count) => (None,13)
13 is labels’ count in our problem. i.e Total number of emotions count.
Softmax is probability distribution activation function and helps 
in achieving better results by distributing probability among labels for a given input.
After adding this, we get 13 Outputs each lying between 0 and 1 for each input. 
Each output represents probability of that emotion for given input. One with highest value can be considered as our prediction.

--------
Conv1D :
--------
CNNs work the same way whether they have 1, 2, or 3 dimensions. 
The difference is the structure of the input data and how the filter, 
also known as a convolution kernel or feature detector, moves across the data.
A 1D CNN is very effective for deriving features from a fixed-length segment 
of the overall dataset, where it is not so important where the feature is located in the segment.
"""

model = Sequential()
model.add(embedding_layer)
model.add(Conv1D(30, 1, activation="relu")) # output size : (None, 20, 30)
model.add(MaxPooling1D(4)) # output size : (None, 5, 50) - 20 / MaxPoolling1D size = 5
model.add(LSTM(100, return_sequences=True)) # the output is the output of the last cell
model.add(Flatten())
model.add(Dense(500, activation='relu'))
model.add(Dense(300, activation='relu'))
model.add(Dense(y_data.shape[1], activation="softmax"))
model.compile(loss="categorical_crossentropy", optimizer="sgd", metrics=["accuracy"])
print(model.summary())


print("Finished Preprocessing data ...")
print("[+] x_data shape >>> ", x_data.shape)
print("[+] y_data shape >>> ", y_data.shape)



# We gotta split our data into two parts, Training Data, Testing Data
# We use Training dataset to train our neural network. 
# Test dataset to provide an unbiased evaluation of a final model fit on the training dataset.
# Split arrays or matrices into random train and test subsets
print("[+] spliting data into training, testing set ...")
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data)


# # x_train
# [[    0     0     0 ...     0    93   866]
#  [    0     0     0 ...  1498  5908  7727]
#  [    0     0     0 ...   565  2943   162]
#  ...
#  [    0     0     0 ...  1056 10331 24539]
#  [    0     0     0 ...   520   995    71]
#  [    0     0     0 ...   565   789   129]] 
# # x_test
# [[    0     0     0 ...    49   595  2060]
#  [    0     0     0 ...  2583  3626  2401]
#  [    0     0     0 ...  3423  2463   398]
#  ...
#  [    0     0     0 ...   124    11 18737]
#  [    0     0     0 ...    69  8389  8389]
#  [    0     0     0 ...    15    27  5723]] 
# # y_train
# [[0. 0. 0. ... 0. 0. 0.]
#  [0. 0. 0. ... 0. 0. 0.]
#  [0. 0. 0. ... 0. 0. 0.]
#  ...
#  [0. 0. 0. ... 0. 0. 1.]
#  [0. 0. 0. ... 0. 0. 0.]
#  [0. 0. 0. ... 0. 0. 0.]] 
# # y_test
# [[0. 0. 0. ... 0. 0. 0.]
#  [0. 0. 0. ... 0. 0. 0.]
#  [0. 0. 0. ... 1. 0. 0.]
#  ...
#  [0. 0. 0. ... 0. 0. 1.]
#  [0. 0. 0. ... 0. 0. 0.]
#  [0. 0. 0. ... 0. 0. 0.]]




batch_size = 64
num_epochs = 100


x_valid, y_valid = x_train[:batch_size], y_train[:batch_size]
x_train2, y_train2 = x_train[batch_size:], y_train[batch_size:]




"""
Save the model after every epoch.
filepath can contain named formatting options, which will be filled 
with the values of epoch and keys in logs (passed in on_epoch_end).
For example: if filepath is weights.{epoch:02d}-{val_loss:.2f}.hdf5, 
then the model checkpoints will be saved with the epoch number and the validation loss in the filename.

Arguments

filepath: string, path to save the model file.
monitor: quantity to monitor.
verbose: verbosity mode, 0 or 1.
save_best_only: if save_best_only=True, the latest best model according 
    to the quantity monitored will not be overwritten.
    save_weights_only: if True, then only the model's weights will be saved 
    (model.save_weights(filepath)), else the full model is saved (model.save(filepath)).
mode: one of {auto, min, max}. If save_best_only=True, the decision to overwrite 
    the current save file is made based on either the maximization or the minimization 
    of the monitored quantity. For val_acc, this should be max, for val_loss this 
    should be min, etc. In auto mode, the direction is automatically inferred 
    from the name of the monitored quantity.
period: Interval (number of epochs) between checkpoints.
"""

st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
print(f"[+] start time to train >>> {st}")

# Now, we define check point conditions, 
# these checkpoints will save our model locally if there’s an improvement.
filepath = "/gdrive/My Drive/ted/ed-{epoch:02d}-{val_acc:.6f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'auto', save_weights_only = False)
callbacks_list = [checkpoint]


# fit x_train and y_train data to our model to train on them based on x_valid and y_valid 
history = model.fit(x_train2, y_train2, validation_data = (x_valid, y_valid), batch_size = batch_size, epochs = num_epochs, callbacks = callbacks_list)
plot_history(history)


ft = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
print(f"[+] stop time for training >>> {ft}")

# evaluate model on the test data 
scores = model.evaluate(x_test, y_test, verbose = 0)
tr_scores = model.evaluate(x_train, y_train, verbose = False)
print("[+] Training Accuracy : {:.6f}".format(tr_scores[1]))
print("[+] Training Loss     : {:.6f}".format(tr_scores[0]))
te_scores = model.evaluate(x_test, y_test, verbose = False)
print("[+] Testing Accuracy  : {:.6f}".format(te_scores[1]))
print("[+] Testing Loss      : {:.6f}".format(tr_scores[0]))

"""# **GETTING EMOTIONS OF OUR TEXT**
*`predicting using our model`*
"""

from keras.preprocessing.text import text_to_word_sequence, Tokenizer
import numpy as np, emojis, nltk
from nltk.corpus import stopwords
from keras.preprocessing.sequence import pad_sequences
from keras.models import load_model
from google.colab import drive
import pickle
drive.mount('/gdrive')
nltk.download('stopwords')

pickle_f = open("/gdrive/My Drive/ted/word_index.pkl","rb")
vocab = pickle.load(pickle_f)


def getEmojis(i):
    
    emojies = {
         0: ':angry:', 1: ':expressionless:', 2: ':no_mouth:', 3: ':heart_eyes:', 
         4: ':smile:', 5: ':blush:', 6: ':-1:', 7: ':heart:' , 8: ':neutral_face:',
         9: ':sweat_smile:', 10: ':pensive:', 11: ':anguished:' , 12: ':worried:'
        }
    
    return emojies[i]


def textToInput(x):
    '''
    fit_on_texts Updates internal vocabulary based on 
    a list of texts. This method creates the vocabulary 
    index based on word frequency. So if you give it something like, 
    "The cat sat on the mat." It will create a dictionary 
    s.t. word_index["the"] = 1; word_index["cat"] = 2 
    it is word -> index dictionary so every word gets 
    a unique integer value. 0 is reserved for padding. 
    So lower integer means more frequent word (often the first few are stop words because they appear a lot).
    
    texts_to_sequences Transforms each text in texts to 
    a sequence of integers. So it basically takes each 
    word in the text and replaces it with its corresponding 
    integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.
    Why don't combine them? Because you almost always fit once 
    and convert to sequences many times. You will fit on your 
    training corpus once and use that exact same  word_index 
    dictionary at train / eval / testing / prediction time to 
    convert actual text into sequences to feed them to the network. 
    So it makes sense to keep those methods separate.
    '''
    
    stop_words = set(stopwords.words('english'))
    new_stop_words = set(stop_words)
    for s in stop_words:
        new_stop_words.add(s.replace('\'',''))
        pass
    stop_words = new_stop_words
    base_filters = '\n\t!"#$%&()*+,-./:;<=>?[\]^_`{|}~ '
    word_sequences = []
    for i in x:
        i = str(i)
        i = i.replace('\'', '')
        newlist = [x for x in text_to_word_sequence(i,filters = base_filters, lower = True) if not x.startswith("@")]
        filtered_sentence = [w for w in newlist if not w in stop_words] 
        word_sequences.append(filtered_sentence)
        pass
   
    word_indices = []
    for seq in word_sequences:
        word_indices.append([vocab[x] for x in seq if x in vocab.keys()])
        
    print("\n[+] word indices before padding ... ")
    print(word_indices)
    
    x_data = pad_sequences(word_indices, maxlen=20)
    print("\n[+] NN input is <<< word indices , 20 length of each sequence after padded >>>: ")
    print(x_data)
    print()
    return x_data



# all these words must be in our vocab which is built based on our dataset
# in this case we have 32855 words in our entire dataset , any word that 
# doesn't exit in there won't have vector(cause the embedding layer has all vector of our vocab)! 
# cause we can't find its index in our vocab.
text = ["can you see the sky, there is a little hole in there!", 
        "unknown sense your toy inside her room before his mother come into me",
        "the red sky just hit me and my face is on fire right now"
       ]

text_mat = np.asarray(text)
NNInput = textToInput(text_mat)
model = load_model('/gdrive/My Drive/ted/ted.hdf5')
prediction = model.predict(NNInput)


# getting the vector of a word according to its index in word_index(vocab) pickle
embeddings = model.layers[0].get_weights()[0]
words_embeddings = {w:embeddings[idx] for w, idx in vocab.items()}
print("vector of love according to its index in our vocab is : \n", words_embeddings['love'])


# -----------
# prediction:
# -----------
# we'll choose the maximum prob to show the emotion of our text
# we can also show every prob to the user in order to analysis his/her text!

# [[0.00285324 0.00443429 0.02685553 0.02307291 0.04944    0.15052915
#   0.01866804 0.06597449 0.2738337  0.04779249 0.10498895 0.06976512
#   0.1617921 ]
#  [0.0013604  0.00217447 0.00926796 0.01718927 0.0403096  0.09397643
#   0.00903581 0.27401027 0.16561657 0.04202646 0.10514135 0.0530719
#   0.18681952]]

for i in prediction:
    print(emojis.encode(f' you are  {getEmojis(np.argmax(i))}'))